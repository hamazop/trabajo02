{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNQVAVTA3lkL"
   },
   "source": [
    "# Trabajo 02: Modelos de riesgo de crédito con RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXJKoB4j3tVx"
   },
   "source": [
    "## Pre-procesamiento de los datos\n",
    "\n",
    "En este segmento explicamos todo el análisis exploratorio de datos, perfilamiento y preparación de los datos para su uso en una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7tOjviQXzHS"
   },
   "source": [
    "Importamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12353,
     "status": "ok",
     "timestamp": 1737831864710,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "yrjNFPalWDTN",
    "outputId": "e9e229db-d876-460f-83ed-6cf53f78e42f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kopynft\\AppData\\Local\\Temp\\ipykernel_12332\\4139838488.py:6: DtypeWarning: Columns (19,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(\"loan.csv\") # Cargar datos como df_raw\n"
     ]
    }
   ],
   "source": [
    "# data set from\n",
    "# https://www.kaggle.com/api/v1/datasets/download/ranadeep/credit-risk-dataset?dataset_version_number=3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(\"loan.csv\") # Cargar datos como df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8TxRAZFdu0F"
   },
   "source": [
    "Leemos las columnas de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1737784696771,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "3PB9bvYPdqmG",
    "outputId": "77ecde4b-5435-4c22-f5f6-7c4d427b00cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
      "       'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title',\n",
      "       'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
      "       'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose',\n",
      "       'title', 'zip_code', 'addr_state', 'dti', 'delinq_2yrs',\n",
      "       'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq',\n",
      "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
      "       'revol_util', 'total_acc', 'initial_list_status', 'out_prncp',\n",
      "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
      "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
      "       'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
      "       'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med',\n",
      "       'mths_since_last_major_derog', 'policy_code', 'application_type',\n",
      "       'annual_inc_joint', 'dti_joint', 'verification_status_joint',\n",
      "       'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m',\n",
      "       'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n",
      "       'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n",
      "       'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl',\n",
      "       'inq_last_12m'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8W1LBbRe4g3"
   },
   "source": [
    "Hacemos un análisis de los datos presentes y decidimos qué datos se van a quitar y cuáles a utilizar para el entrenamiento de la red neuronal. La siguiente información es extraída del mismo sitio de donde se obtuvieron los [datos](https://www.kaggle.com/datasets/ranadeep/credit-risk-dataset/data). En primera instancia se ignoran las columnas que tienen muchos datos nulos, se considera \"muchos\" a aquellas columnas que tengan más de 50 nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1737831870186,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "iOh2WgLVhgnO",
    "outputId": "9435287c-50ea-4142-99fb-520c479c9246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en cada columna:\n",
      "id                       0\n",
      "member_id                0\n",
      "loan_amnt                0\n",
      "funded_amnt              0\n",
      "funded_amnt_inv          0\n",
      "                     ...  \n",
      "all_util            866007\n",
      "total_rev_hi_lim     70276\n",
      "inq_fi              866007\n",
      "total_cu_tl         866007\n",
      "inq_last_12m        866007\n",
      "Length: 74, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = df_raw.isnull().sum() # Sumar nulos por columna\n",
    "\n",
    "print(\"Nulos en cada columna:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udgZPunPksTp"
   },
   "source": [
    "Se eliminan entonces las siguientes columnas:\n",
    "\n",
    "\n",
    "\"emp_title\", \"emp_length\", \"desc\", \"title\", \"mths_since_last_delinq\", \"mths_since_last_record\", \"revol_util\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\", \"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\", \"tot_coll_amt\", \"tot_cur_bal\", \"open_acc_6m\", \"open_il_6m\", \"open_il_12m\", \"open_il_24m\", \"mths_since_rcnt_il\", \"total_bal_il\", \"il_util\", \"open_rv_12m\", \"open_rv_24m\", \"max_bal_bc\", \"all_util\", \"total_rev_hi_lim\", \"inq_fi\", \"total_cu_tl\", \"inq_last_12m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1737831874738,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "lQX6PBmFkX4s"
   },
   "outputs": [],
   "source": [
    "df_nonull = df_raw.drop([\"emp_title\", \"emp_length\", \"desc\", \"title\", \"mths_since_last_delinq\", \"mths_since_last_record\", \"revol_util\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\", \"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\", \"tot_coll_amt\", \"tot_cur_bal\", \"open_acc_6m\", \"open_il_6m\", \"open_il_12m\", \"open_il_24m\", \"mths_since_rcnt_il\", \"total_bal_il\", \"il_util\", \"open_rv_12m\", \"open_rv_24m\", \"max_bal_bc\", \"all_util\", \"total_rev_hi_lim\", \"inq_fi\", \"total_cu_tl\", \"inq_last_12m\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTWGlJACoZ-D"
   },
   "source": [
    "Nos quedan ahora las siguientes columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1737831876897,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "A_8p4bHdoZTn",
    "outputId": "4af4892e-7832-42d4-bb72-7631427861c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
      "       'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
      "       'home_ownership', 'annual_inc', 'verification_status', 'issue_d',\n",
      "       'loan_status', 'pymnt_plan', 'url', 'purpose', 'zip_code', 'addr_state',\n",
      "       'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'open_acc',\n",
      "       'pub_rec', 'revol_bal', 'total_acc', 'initial_list_status', 'out_prncp',\n",
      "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
      "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
      "       'collection_recovery_fee', 'last_pymnt_amnt', 'policy_code',\n",
      "       'application_type', 'acc_now_delinq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_nonull.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "587F5GqDhX2d"
   },
   "source": [
    "De esas columnas se decide eliminar entonces información considerada no significativa, ya sea porque es información personal, implica añadir complejidad financiera a la red, se considera redundante o aplica solo a deudas que se consideran \"perdidas\". Se eliminan:\n",
    "\n",
    "addr_state: The state provided by the borrower in the loan application.\\\n",
    "collection_recovery_fee: Post charge off collection fee.\\\n",
    "funded_amnt: The total amount committed to that loan at that point in time.\\\n",
    "funded_amnt_inv: The total amount committed by investors for that loan at that point in time.\\\n",
    "grade: LC assigned loan grade.\\\n",
    "id: A unique LC assigned ID for the loan listing.\\\n",
    "initial_list_status: The initial listing status of the loan. Possible values are W, F.\\\n",
    "issue_d: The month which the loan was funded.\\\n",
    "member_id: A unique LC assigned Id for the borrower member.\\\n",
    "out_prncp: Remaining outstanding principal for total amount funded.\\\n",
    "out_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors.\\\n",
    "pymnt_plan: Indicates if a payment plan has been put in place for the loan.\\\n",
    "recoveries: post charge off gross recovery\\\n",
    "total_pymnt: Payments received to date for total amount funded.\\\n",
    "total_pymnt_inv: Payments received to date for portion of total amount funded by investors.\\\n",
    "total_rec_int: Interest received to date.\\\n",
    "total_rec_late_fee: Late fees received to date.\\\n",
    "total_rec_prncp: Principal received to date.\\\n",
    "url: URL for the LC page with listing data.\\\n",
    "verification_status: NO APARECE EN EL DICCIONARIO.\\\n",
    "zip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n",
    "\n",
    "\"addr_state\", \"collection_recovery_fee\", \"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"id\", \"initial_list_status\", \"issue_d\", \"member_id\", \"out_prncp\", \"out_prncp_inv\", \"pymnt_plan\", \"recoveries\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_int\", \"total_rec_late_fee\", \"total_rec_prncp\", \"url\", \"verification_status\", \"zip_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1737831881031,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "FVzsPJEiry75"
   },
   "outputs": [],
   "source": [
    "df_clean = df_nonull.drop([\"addr_state\", \"collection_recovery_fee\", \"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"id\", \"initial_list_status\", \"issue_d\", \"member_id\", \"out_prncp\", \"out_prncp_inv\", \"pymnt_plan\", \"recoveries\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_int\", \"total_rec_late_fee\", \"total_rec_prncp\", \"url\", \"verification_status\", \"zip_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DCP5NJ4yNCv"
   },
   "source": [
    "Quedan entonces las siguientes columans:\n",
    "\n",
    "acc_now_delinq: The number of accounts on which the borrower is now delinquent.\\\n",
    "application_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers.\\\n",
    "annual_inc: The self-reported annual income provided by the borrower during registration.\\\n",
    "delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past [...]\\\n",
    "dti: A ratio calculated using the borrower's total monthly debt payments on the total debt obligations [...]\\\n",
    "earliest_cr_line: The date the borrower's earliest reported credit line was opened.\\\n",
    "home_ownership: The home ownership status provided by the borrower during registration.\\\n",
    "inq_last_6mths: The number of inquiries in past 6 months (excluding auto and mortgage inquiries).\\\n",
    "installment: The monthly payment owed by the borrower if the loan originates.\\\n",
    "int_rate: Interest Rate on the loan.\\\n",
    "last_pymnt_amnt: Last total payment amount received.\\\n",
    "loan_amnt: The listed amount of the loan applied for by the borrower.\\\n",
    "loan_status: Current status of the loan.\\\n",
    "open_acc: The number of open credit lines in the borrower's credit file.\\\n",
    "policy_code: publicly available policy_code=1 new products not publicly available policy_code=2.\\\n",
    "pub_rec: Number of derogatory public records.\\\n",
    "purpose: A category provided by the borrower for the loan request.\\\n",
    "revol_bal: Total credit revolving balance.\\\n",
    "sub_grade: LC assigned loan subgrade.\\\n",
    "term: The number of payments on the loan. Values are in months and can be either 36 or 60.\\\n",
    "total_acc: The total number of credit lines currently in the borrower's credit file.\\\n",
    "\n",
    "Veamos ahora los tipos de datos, ya que algunos como \"earliest_cr_line\" no son de tipo numérico sino de tipo fecha, así que deben utilizarse una conversión para facilitar el entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-M9U4zlWb2Q"
   },
   "source": [
    "Para las columnas: term, sub_grade, home_ownership, loan_status, purpose, earliest_cr_line y application_type se debe hacer un procesamiento externo para convertirlos a datos más sencillos.\n",
    "\n",
    "Para \"term\" que son las cuotas, vienen en \"36 months\" y \"60 months\" así que se retirará la parte de tipo texto y se dejará solo el valor numérico.\n",
    "\n",
    "\"sub_grade\" cuenta con las categorías de A a G y sub categorías de 1 a 5, así que se cambiarán consecuentemente desde A1 hasta G5 por valores numéricos del 1 al 35 (A1: 1, A2: 2, ..., B3: 8, B4: 9, ..., G5: 35).\n",
    "\n",
    "\"home_ownsership\" especifica si quien solicita la deuda posee casa, tiene las categorías: Mortgage (Hipoteca), Rent (Arrienda), Own (Propia), Other (Otra), None (Ninguna). Nos interesan los criterios de si posee o no y si tiene arrendamiento, así que se hará la siguiente conversión.\n",
    "\n",
    "Mortgage: 0 porque la persona tiene una deuda que pagar.\\\n",
    "Own: 2 porque la persona posee vivienda propia y no debe gastar en arriendo.\\\n",
    "Los demás se les asigna un valor de 1, para posicionarlos en medio.\n",
    "\n",
    "Respecto a \"purpose\" se les asignará valores de 0 a 13 del criterio más repetido al menos repetido (debt_consolidation: 0, credit_card: 1, home_improvement: 2, other: 3, major_purchase: 4\n",
    "small_business: 5, car: 6, medical: 7, moving: 8, vacation: 9, house: 10, wedding: 11, renewable_energy: 12, educational: 13)\n",
    "\n",
    "\"earliest_cr_line\" es un dato que originalmente está en tipo fecha así que decide separarse el año y el mes para guardarlos como tipo numérico\n",
    "\n",
    "Por otro lado \"application_type\" tiene el criterio \"INDIVIDUAL\" y \"JOINT\" por lo que convertimos INDIVIDUAL a 0 y JOINT a 1\n",
    "\n",
    "Finalmente \"loan_status\" es la variable que nos interesa ya que estamos midiendo la capacidad de un cliente de pagar el crédito, así que le cambiaremos el nombre a \"target\" asignaremos los siguientes valores:\n",
    "\n",
    "Fully Paid y Current: 1 puesto que signifca que han cumplido o están cumpliendo con los pagos.\\\n",
    "Los demás valores se les asignará 0 ya que si bien no la han pagado ni están al día, tampoco se considera perdida así que todavía puede que paguen la deuda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4572,
     "status": "ok",
     "timestamp": 1737831893882,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "E6pFpmlIWsPD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Primero extraemos la parte numérica del \"term\"\n",
    "df_clean[\"term\"] = df_clean[\"term\"].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# 2. Ajustamos \"subgrade\"\n",
    "# Creamos un diccionario\n",
    "mapeo_subgrade = { 'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5,\n",
    "                   'B1': 6, 'B2': 7, 'B3': 8, 'B4': 9, 'B5': 10,\n",
    "                   'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, 'C5': 15,\n",
    "                   'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20,\n",
    "                   'E1': 21, 'E2': 22, 'E3': 23, 'E4': 24, 'E5': 25,\n",
    "                   'F1': 26, 'F2': 27, 'F3': 28, 'F4': 29, 'F5': 30,\n",
    "                   'G1': 31, 'G2': 32, 'G3': 33, 'G4': 34, 'G5': 35,\n",
    "}\n",
    "\n",
    "# Mapeamos los \"subgrades\" a sus valores numéricos\n",
    "df_clean[\"sub_grade\"] = df_clean[\"sub_grade\"].map(mapeo_subgrade)\n",
    "\n",
    "# 3. Ajustamos \"home_ownership\"\n",
    "#  Creamos un diccionario\n",
    "mapeo_house = {\n",
    "    'MORTGAGE': 0,\n",
    "    'OWN': 2,\n",
    "    'other': 1  # Asignamos un valor por defecto a las demás categorías\n",
    "}\n",
    "\n",
    "\n",
    "# Mapeamos las categorías a sus valores numéricos\n",
    "df_clean[\"home_ownership\"] = df_clean[\"home_ownership\"].map(mapeo_house).fillna(1)\n",
    "\n",
    "# 4. Ajustamos \"purpose\"\n",
    "# Creamos un diccionario\n",
    "mapeo_purpose = { \"debt_consolidation\": 0, \"credit_card\": 1,\n",
    "                  \"home_improvement\": 2, \"other\": 3, \"major_purchase\": 4,\n",
    "                  \"small_business\": 5, \"car\": 6, \"medical\": 7, \"moving\": 8,\n",
    "                  \"vacation\": 9, \"house\": 10, \"wedding\": 11,\n",
    "                  \"renewable_energy\": 12, \"educational\": 13\n",
    "}\n",
    "# Mapeamos los \"pupose\" a sus valores numéricos\n",
    "df_clean[\"purpose\"] = df_clean[\"purpose\"].map(mapeo_purpose)\n",
    "\n",
    "# 5. Ajustamos \"earliest_cr_line\"\n",
    "# Convertimos de tipo Object a tipo Fecha con Mes-Año\n",
    "df_clean[\"earliest_cr_line\"] = pd.to_datetime(df_clean[\"earliest_cr_line\"], format='%b-%Y')\n",
    "\n",
    "# Extraemos el mes y el año\n",
    "df_clean[\"month\"] = df_clean[\"earliest_cr_line\"].dt.month\n",
    "df_clean[\"year\"] = df_clean[\"earliest_cr_line\"].dt.year\n",
    "\n",
    "# Finalmente ignoramos la columna original\n",
    "df_clean = df_clean.drop(\"earliest_cr_line\", axis=1)\n",
    "\n",
    "# 6. Ajustamos \"application_type\"\n",
    "# Creamos un diccionario\n",
    "mapeo_aptype = { \"INDIVIDUAL\": 0,  \"JOINT\": 1}\n",
    "\n",
    "# Mapeamos los \"application_type\" a sus valores numéricos\n",
    "df_clean[\"application_type\"] = df_clean[\"application_type\"].map(mapeo_aptype)\n",
    "\n",
    "# 7. Ajustamos y renombramos \"loan_status\"\n",
    "# Creamos un diccionario\n",
    "mapeo_lnst = {\"Current\": 1,\n",
    "              \"Fully Paid\": 1,\n",
    "              \"other\": 0}\n",
    "\n",
    "# Mapeamos los \"loan_status\" a sus valores numéricos\n",
    "df_clean[\"loan_status\"] = df_clean[\"loan_status\"].map(mapeo_lnst).fillna(0)\n",
    "\n",
    "# Cambiamos el tipo a booleano\n",
    "df_clean[\"loan_status\"] = df_clean[\"loan_status\"].astype(bool) \n",
    "\n",
    "# Y finalmente renombramos\n",
    "df_clean = df_clean.rename(columns={\"loan_status\": 'target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOkumGJQ9TdB"
   },
   "source": [
    "Finalmente quitamos los NaN que hayan quedado en las filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1737831908017,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "3OV1rzYa9Wz-"
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT81NVUdzSqH"
   },
   "source": [
    "Con esto hemos concluido todo los pre-procesamientos necesarios a la información original y ahora podemos prepararla para entrenar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ehWauxs4f92"
   },
   "source": [
    "## Procesamiento de los datos y entreamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-_ImPdwGl8M"
   },
   "source": [
    "Dado que existe un gran desbalance entre la información resultante, puesto que existe criterios muy diferentes entre quienes logran pagar y quienes no, así mismo con la información que se tiene de ellos (por ejemplo hay muy pocos quienes tienen casa o quienes han logrado existosamente pagar su créditos) así que haremos una técnica de balanceo llamada Tomek Link permitiendo una mejor identificación, adicionalmente se usa SMOTE para darle una mejor representación a las categorías minoritarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 5884201,
     "status": "error",
     "timestamp": 1737837795538,
     "user": {
      "displayName": "Hugo Andres Mazo Pacheco",
      "userId": "07546042169703840818"
     },
     "user_tz": 300
    },
    "id": "-scc0rdi6Ee6",
    "outputId": "f4f83df8-f096-46c2-a6eb-a22990925fb8"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separar características y target\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "# Submuestreo utilizando TomekLinks para eliminar muestras cercanas\n",
    "tl = TomekLinks()\n",
    "X_under, y_under = tl.fit_resample(X, y)\n",
    "\n",
    "# Sobremuestreo de la clase minoritaria hasta el 30% de la clase mayoritaria\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state=40)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "# Combinar las características balanceadas con el target\n",
    "df_training = pd.concat([pd.DataFrame(X_balanced, columns=X.columns), pd.Series(y_balanced, name='target')], axis=1)\n",
    "\n",
    "# Guardamos los datasets para posterior uso\n",
    "df_clean.to_csv(\"datos_limpios.csv\", index=False)\n",
    "df_training.to_csv(\"datos_balanceados.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después del ajuste y el balanceo de datos se procede ahora a el entreamiento del modelo. La red neuronal se compone de dos capas de 32 y 16 neuronas con activación relu con la capa de salida de una neurona con una activación sigmoide, la compilación del modelo está compuesta por el optimizador Adaptive Moment Estimation, la función de perdida es binary_crossentropy, útil en este caso por ser una clasificación binaria, y las métricas de optimización se decidió por el ROC siglas de área bajo la curva, PR Calcula el área bajo la curva de precisión-recall, útil en problemas con clases desbalanceadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv(\"datos_balanceados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kopynft\\Documents\\UNAL\\RNAABI\\Trabajo 02\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step - auc: 0.8291 - loss: 0.5015 - pr_auc: 0.9394\n",
      "Epoch 2/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2ms/step - auc: 0.8902 - loss: 0.4129 - pr_auc: 0.9609\n",
      "Epoch 3/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2ms/step - auc: 0.9045 - loss: 0.3849 - pr_auc: 0.9661\n",
      "Epoch 4/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2ms/step - auc: 0.9093 - loss: 0.3735 - pr_auc: 0.9679\n",
      "Epoch 5/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - auc: 0.9117 - loss: 0.3681 - pr_auc: 0.9687\n",
      "Epoch 6/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - auc: 0.9141 - loss: 0.3628 - pr_auc: 0.9695\n",
      "Epoch 7/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - auc: 0.9167 - loss: 0.3573 - pr_auc: 0.9704\n",
      "Epoch 8/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - auc: 0.9187 - loss: 0.3524 - pr_auc: 0.9712\n",
      "Epoch 9/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - auc: 0.9203 - loss: 0.3489 - pr_auc: 0.9718\n",
      "Epoch 10/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9211 - loss: 0.3471 - pr_auc: 0.9721\n",
      "Epoch 11/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - auc: 0.9209 - loss: 0.3474 - pr_auc: 0.9720\n",
      "Epoch 12/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - auc: 0.9225 - loss: 0.3441 - pr_auc: 0.9725\n",
      "Epoch 13/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9239 - loss: 0.3409 - pr_auc: 0.9731\n",
      "Epoch 14/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9245 - loss: 0.3394 - pr_auc: 0.9732\n",
      "Epoch 15/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - auc: 0.9243 - loss: 0.3393 - pr_auc: 0.9732\n",
      "Epoch 16/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - auc: 0.9251 - loss: 0.3377 - pr_auc: 0.9735\n",
      "Epoch 17/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - auc: 0.9257 - loss: 0.3364 - pr_auc: 0.9736\n",
      "Epoch 18/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - auc: 0.9260 - loss: 0.3351 - pr_auc: 0.9739\n",
      "Epoch 19/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - auc: 0.9261 - loss: 0.3362 - pr_auc: 0.9737\n",
      "Epoch 20/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - auc: 0.9268 - loss: 0.3342 - pr_auc: 0.9739\n",
      "Epoch 21/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9272 - loss: 0.3330 - pr_auc: 0.9743\n",
      "Epoch 22/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - auc: 0.9284 - loss: 0.3291 - pr_auc: 0.9746\n",
      "Epoch 23/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9292 - loss: 0.3272 - pr_auc: 0.9748\n",
      "Epoch 24/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - auc: 0.9293 - loss: 0.3270 - pr_auc: 0.9748\n",
      "Epoch 25/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.9296 - loss: 0.3261 - pr_auc: 0.9749\n",
      "Epoch 26/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - auc: 0.9296 - loss: 0.3259 - pr_auc: 0.9749\n",
      "Epoch 27/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - auc: 0.9302 - loss: 0.3247 - pr_auc: 0.9752\n",
      "Epoch 28/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - auc: 0.9303 - loss: 0.3242 - pr_auc: 0.9752\n",
      "Epoch 29/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - auc: 0.9300 - loss: 0.3251 - pr_auc: 0.9750\n",
      "Epoch 30/30\n",
      "\u001b[1m25197/25197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - auc: 0.9308 - loss: 0.3235 - pr_auc: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a83ccc5b10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "# Definir y compilar el modelo\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Supongamos que dEntrenamiento_balanced contiene todas las columnas\n",
    "X = df_training.drop(\"target\", axis=1)\n",
    "y = df_training[\"target\"]\n",
    "\n",
    "# Normalización opcional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División de datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calcular pesos de clase para manejar la clase minoritaria\n",
    "classes = np.unique(y_train)\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\", input_shape=(X_scaled.shape[1],)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[keras.metrics.AUC(name=\"auc\", curve=\"ROC\"),keras.metrics.AUC(name=\"pr_auc\", curve=\"PR\")])\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_train = pd.DataFrame(X_train).reset_index(drop=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 768us/step\n",
      "0.8541195380402429 0.9371652360216475 0.8686443409973049 0.9016047862515727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_classes = (y_pred_probs >= 0.5).astype(int)  # Ajustar umbral\n",
    "acc = accuracy_score(y_test, y_pred_classes)\n",
    "prec = precision_score(y_test, y_pred_classes)\n",
    "rec = recall_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "print(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Probabilidad predicción: [[0.9448362]]\n",
      "Clase predicha: [[1]]\n"
     ]
    }
   ],
   "source": [
    "fila_ejemplo = X_test[0:1]  # Toma una fila como ejemplo\n",
    "prob_pred = model.predict(fila_ejemplo)\n",
    "clase_pred = (prob_pred >= 0.5).astype(int)\n",
    "\n",
    "print(\"Probabilidad predicción:\", prob_pred)\n",
    "print(\"Clase predicha:\", clase_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31497/31497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 734us/step\n",
      "0.8544972705029775 0.9372784538933685 0.8689988662321632 0.9018481322907457\n"
     ]
    }
   ],
   "source": [
    "# Datos de entrada\n",
    "X = df_training.drop(\"target\", axis=1)\n",
    "y = df_training[\"target\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_pred_probs = model.predict(X_scaled)\n",
    "y_pred_classes = (y_pred_probs >= 0.5).astype(int)  # Ajustar umbral\n",
    "acc = accuracy_score(y, y_pred_classes)\n",
    "prec = precision_score(y, y_pred_classes)\n",
    "rec = recall_score(y, y_pred_classes)\n",
    "f1 = f1_score(y, y_pred_classes)\n",
    "print(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kopynft\\Documents\\UNAL\\RNAABI\\Trabajo 02\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - auc: 0.8173 - loss: 0.5169 - pr_auc: 0.9352 - val_auc: 0.8708 - val_loss: 0.4666 - val_pr_auc: 0.9547\n",
      "Epoch 2/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step - auc: 0.8752 - loss: 0.4384 - pr_auc: 0.9565 - val_auc: 0.8872 - val_loss: 0.3962 - val_pr_auc: 0.9605\n",
      "Epoch 3/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - auc: 0.8924 - loss: 0.4084 - pr_auc: 0.9622 - val_auc: 0.9046 - val_loss: 0.3703 - val_pr_auc: 0.9663\n",
      "Epoch 4/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - auc: 0.9052 - loss: 0.3826 - pr_auc: 0.9669 - val_auc: 0.9100 - val_loss: 0.3438 - val_pr_auc: 0.9682\n",
      "Epoch 5/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - auc: 0.9091 - loss: 0.3743 - pr_auc: 0.9680 - val_auc: 0.9102 - val_loss: 0.3742 - val_pr_auc: 0.9681\n",
      "Epoch 6/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - auc: 0.9115 - loss: 0.3691 - pr_auc: 0.9687 - val_auc: 0.9143 - val_loss: 0.3456 - val_pr_auc: 0.9695\n",
      "Epoch 7/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - auc: 0.9141 - loss: 0.3639 - pr_auc: 0.9694 - val_auc: 0.9130 - val_loss: 0.3507 - val_pr_auc: 0.9691\n",
      "Epoch 8/40\n",
      "\u001b[1m20158/20158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - auc: 0.9149 - loss: 0.3613 - pr_auc: 0.9697 - val_auc: 0.9176 - val_loss: 0.3563 - val_pr_auc: 0.9707\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 843us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 786us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 898us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 872us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 877us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 824us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 773us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 774us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 776us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 788us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 778us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 799us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 792us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 800us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 823us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 795us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 817us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 811us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 834us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 836us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 919us/step\n",
      "\u001b[1m6300/6300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
      "\n",
      "Influencia de cada variable (Ordenadas de mayor a menor impacto):\n",
      "Variable: last_pymnt_amnt | ΔROC-AUC: 0.1815 | ΔPR-AUC: 0.1100\n",
      "Variable: int_rate | ΔROC-AUC: 0.1554 | ΔPR-AUC: 0.0640\n",
      "Variable: loan_amnt | ΔROC-AUC: 0.1050 | ΔPR-AUC: 0.0338\n",
      "Variable: sub_grade | ΔROC-AUC: 0.1038 | ΔPR-AUC: 0.0408\n",
      "Variable: term | ΔROC-AUC: 0.0621 | ΔPR-AUC: 0.0198\n",
      "Variable: installment | ΔROC-AUC: 0.0486 | ΔPR-AUC: 0.0165\n",
      "Variable: home_ownership | ΔROC-AUC: 0.0425 | ΔPR-AUC: 0.0118\n",
      "Variable: inq_last_6mths | ΔROC-AUC: 0.0097 | ΔPR-AUC: 0.0038\n",
      "Variable: pub_rec | ΔROC-AUC: 0.0096 | ΔPR-AUC: 0.0028\n",
      "Variable: annual_inc | ΔROC-AUC: 0.0024 | ΔPR-AUC: 0.0008\n",
      "Variable: total_acc | ΔROC-AUC: 0.0021 | ΔPR-AUC: 0.0008\n",
      "Variable: open_acc | ΔROC-AUC: 0.0020 | ΔPR-AUC: 0.0008\n",
      "Variable: revol_bal | ΔROC-AUC: 0.0017 | ΔPR-AUC: 0.0007\n",
      "Variable: dti | ΔROC-AUC: 0.0015 | ΔPR-AUC: 0.0006\n",
      "Variable: acc_now_delinq | ΔROC-AUC: 0.0008 | ΔPR-AUC: 0.0002\n",
      "Variable: year | ΔROC-AUC: 0.0007 | ΔPR-AUC: 0.0003\n",
      "Variable: purpose | ΔROC-AUC: 0.0007 | ΔPR-AUC: 0.0003\n",
      "Variable: month | ΔROC-AUC: 0.0007 | ΔPR-AUC: 0.0002\n",
      "Variable: delinq_2yrs | ΔROC-AUC: 0.0005 | ΔPR-AUC: 0.0002\n",
      "Variable: policy_code | ΔROC-AUC: 0.0000 | ΔPR-AUC: 0.0000\n",
      "Variable: application_type | ΔROC-AUC: -0.0001 | ΔPR-AUC: -0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Supongamos que dEntrenamiento_balanced contiene todas las columnas\n",
    "X = df_training.drop(\"target\", axis=1)\n",
    "y = df_training[\"target\"]\n",
    "\n",
    "# Normalización opcional\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calcular pesos de clase para manejar la clase minoritaria\n",
    "classes = np.unique(y_train)\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Convertir X_train a DataFrame\n",
    "X_train = pd.DataFrame(X_train).reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Definir y compilar el modelo sin cambiar las métricas existentes\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.AUC(name=\"auc\", curve=\"ROC\"),\n",
    "        keras.metrics.AUC(name=\"pr_auc\", curve=\"PR\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Limitar los epochs cuando no mejore (monitor: val_loss)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con early stopping\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    class_weight=class_weights,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluar la influencia de cada variable (Permutation Feature Importance)\n",
    "# ---------------------------\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns).reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Función para evaluar rendimiento (ROC-AUC, PR-AUC)\n",
    "def evaluate_performance(model, X_data, y_data):\n",
    "    y_probs = model.predict(X_data).ravel()\n",
    "    roc = roc_auc_score(y_data, y_probs)\n",
    "    pr = average_precision_score(y_data, y_probs)\n",
    "    return roc, pr\n",
    "\n",
    "# Rendimiento base\n",
    "baseline_roc, baseline_pr = evaluate_performance(model, X_test_df, y_test)\n",
    "\n",
    "feature_importances = []\n",
    "\n",
    "for col in X_test_df.columns:\n",
    "    # Guardar la columna original\n",
    "    original_col = X_test_df[col].copy()\n",
    "\n",
    "    # Barajar (shuffle) la columna\n",
    "    X_test_df[col] = np.random.permutation(X_test_df[col].values)\n",
    "\n",
    "    # Evaluar el rendimiento tras el shuffle\n",
    "    shuffled_roc, shuffled_pr = evaluate_performance(model, X_test_df, y_test)\n",
    "\n",
    "    # Restaurar la columna original\n",
    "    X_test_df[col] = original_col\n",
    "\n",
    "    # Cambio en rendimiento\n",
    "    drop_roc = baseline_roc - shuffled_roc\n",
    "    drop_pr = baseline_pr - shuffled_pr\n",
    "\n",
    "    feature_importances.append((col, drop_roc, drop_pr))\n",
    "\n",
    "# Ordenar de mayor impacto (peor si se shufflea) a menor\n",
    "feature_importances.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "print(\"\\nInfluencia de cada variable (Ordenadas de mayor a menor impacto):\")\n",
    "for feat, d_roc, d_pr in feature_importances:\n",
    "    print(f\"Variable: {feat} | ΔROC-AUC: {d_roc:.4f} | ΔPR-AUC: {d_pr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch, Objective\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datos de entrada\n",
    "X = df_training.drop(\"target\", axis=1)\n",
    "y = df_training[\"target\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustar pesos de clase\n",
    "classes = np.unique(y_train)\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Convertir a DataFrame y Series con índices reiniciados\n",
    "X_train = pd.DataFrame(X_train).reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Definición de función de construcción de modelo para Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    # Número de capas ocultas\n",
    "    for i in range(hp.Int(\"num_layers\", min_value=1, max_value=3)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(\"units_\" + str(i), min_value=16, max_value=128, step=16),\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=keras.regularizers.l2(hp.Float(\"l2_\" + str(i), 0.0001, 0.01, sampling=\"log\"))\n",
    "        ))\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_\" + str(i), 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Tasa de aprendizaje variable\n",
    "    lr = hp.Choice(\"learning_rate\", values=[1e-3, 5e-4, 1e-4])\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[keras.metrics.AUC(name=\"auc\", curve=\"ROC\"),\n",
    "        keras.metrics.AUC(name=\"pr_auc\", curve=\"PR\")])\n",
    "    return model\n",
    "\n",
    "# Configuración de la búsqueda aleatoria\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"my_tuner_dir\",\n",
    "    project_name=\"hyperparam_opt\"\n",
    ")\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early],\n",
    "    class_weight=class_weights,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Obtener el mejor modelo y guardarlo\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save(\"modelo_final.keras\")\n",
    "\n",
    "# Imprimir la arquitectura del mejor modelo\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/SMTt77LuylsZgc6en5cR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Trabajo 02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
